{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d704da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import train_loader, test_loader\n",
    "import torch, transformers\n",
    "from diffusers import StableDiffusionXLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.0+cu126\n",
      "diffusers: 0.33.1\n",
      "transformers: 4.52.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04d303159cd49deab12559114c387dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f386f5c84964be8a33dbec9dddfa91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd035566baf461882806d1cbdfe97e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1254612550b4b59af39c0976e4580c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5dfe375cce4483972f1a71c62da4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8133bd36b1a24bd891a6f73c68854288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d9f599a1864142a57d44f31063edba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6706f31f41e4a4a9ada0ec3832f2d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a17080535045dfb8e8e907943a97e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/5.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4965abac3d64a0dad915a53cd6b5a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db064ac332d84029a6aa11dcb2a896be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b584400da22a423ca5b34260aa07a0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52d7b87cec344ff842230df5879072c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components: ['vae', 'text_encoder', 'text_encoder_2', 'tokenizer', 'tokenizer_2', 'unet', 'scheduler', 'image_encoder', 'feature_extractor']\n",
      "\\nUNet:\n",
      " UNet2DConditionModel(\n",
      "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (add_time_proj): Timesteps()\n",
      "  (add_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=2816, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0-1): 2 x BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=2048, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=2048, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0-9): 10 x BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_blocks): ModuleList(\n",
      "    (0): CrossAttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0-9): 10 x BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0-1): 2 x BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=2048, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=2048, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2DCrossAttn(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Transformer2DModel(\n",
      "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (transformer_blocks): ModuleList(\n",
      "          (0-9): 10 x BasicTransformerBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn1): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn2): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=2048, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "\\nVAE:\n",
      " AutoencoderKL(\n",
      "  (encoder): Encoder(\n",
      "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_blocks): ModuleList(\n",
      "      (0): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_blocks): ModuleList(\n",
      "      (0-1): 2 x UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\\nText Encoder 1:\n",
      " CLIPTextModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 768)\n",
      "      (position_embedding): Embedding(77, 768)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\\nText Encoder 2:\n",
      " CLIPTextModelWithProjection(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 1280)\n",
      "      (position_embedding): Embedding(77, 1280)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (text_projection): Linear(in_features=1280, out_features=1280, bias=False)\n",
      ")\n",
      "\\nTokenizer:\n",
      " CLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "UNet total params: 2,567,463,684\n",
      "UNet trainable params: 2,567,463,684\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Environment Info ---\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"diffusers:\", diffusers.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "\n",
    "# Load SDXL pipeline with fp16 weights\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# --- 3. Inspect Components ---\n",
    "print(\"Pipeline components:\", list(pipe.components.keys()))\n",
    "\n",
    "print(\"\\\\nUNet:\\n\", pipe.unet)\n",
    "print(\"\\\\nVAE:\\n\", pipe.vae)\n",
    "print(\"\\\\nText Encoder 1:\\n\", pipe.text_encoder)\n",
    "print(\"\\\\nText Encoder 2:\\n\", pipe.text_encoder_2)\n",
    "print(\"\\\\nTokenizer:\\n\", pipe.tokenizer)\n",
    "\n",
    "# --- 4. Count Parameters ---\n",
    "total_params = sum(p.numel() for p in pipe.unet.parameters())\n",
    "trainable_params = sum(p.numel() for p in pipe.unet.parameters() if p.requires_grad)\n",
    "print(f\"UNet total params: {total_params:,}\")\n",
    "print(f\"UNet trainable params: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bd5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 140 attention processors to check...\n",
      "Processing: down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.0.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.0.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.0.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.0.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.1.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.1.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.1.attentions.1.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.1.attentions.1.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.0.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.0.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: down_blocks.2.attentions.1.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at down_blocks.2.attentions.1.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.0.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.0.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.1.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.1.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.0.attentions.2.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.0.attentions.2.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.0.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.0.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.0.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.0.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.1.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.1.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.1.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.1.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.2.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.2.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: up_blocks.1.attentions.2.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at up_blocks.1.attentions.2.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.0.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.0.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.0.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.0.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.1.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.1.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.1.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.1.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.2.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.2.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.2.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.2.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.3.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.3.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.3.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.3.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.4.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.4.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.4.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.4.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.5.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.5.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.5.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.5.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.6.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.6.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.6.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.6.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.7.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.7.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.7.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.7.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.8.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.8.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.8.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.8.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.9.attn1.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.9.attn1.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "Processing: mid_block.attentions.0.transformer_blocks.9.attn2.processor\n",
      "❌ Failed to inject LoRA at mid_block.attentions.0.transformer_blocks.9.attn2.processor: LoRAAttnProcessor.__init__() got an unexpected keyword argument 'hidden_size'\n",
      "\n",
      "Injecting 0 processors out of 140\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in processor count – some injections failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ LoRA processors injected successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Try injection with debug tracing\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43minject_lora_with_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m, in \u001b[0;36minject_lora_with_debug\u001b[0;34m(unet, r, alpha)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInjecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(attn_processors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processors out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unet\u001b[38;5;241m.\u001b[39mattn_processors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attn_processors) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unet\u001b[38;5;241m.\u001b[39mattn_processors):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in processor count – some injections failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m unet\u001b[38;5;241m.\u001b[39mset_attn_processor(attn_processors)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ LoRA processors injected successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in processor count – some injections failed."
     ]
    }
   ],
   "source": [
    "# Minimal safe LoRA injection: inject into a single known attention block\n",
    "\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "# Target 1 specific known block: down_blocks[1].attentions[0].transformer_blocks[0].attn2\n",
    "attn_module = pipe.unet.down_blocks[1].attentions[0].transformer_blocks[0].attn2\n",
    "\n",
    "# Extract dimensions\n",
    "hidden_size = attn_module.to_q.in_features\n",
    "cross_attention_dim = attn_module.to_k.in_features\n",
    "\n",
    "# Create LoRA processor and apply\n",
    "lora_processor = LoRAAttnProcessor(\n",
    "    hidden_size=hidden_size,\n",
    "    cross_attention_dim=cross_attention_dim,\n",
    "    rank=4,\n",
    "    scale=1.0,\n",
    ")\n",
    "\n",
    "attn_module.set_processor(lora_processor)\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in attn_module.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in attn_module.parameters())\n",
    "trainable_pct = round(100 * trainable_params / total_params, 4)\n",
    "\n",
    "{\n",
    "    \"Module path\": \"down_blocks[1].attentions[0].transformer_blocks[0].attn2\",\n",
    "    \"Hidden size\": hidden_size,\n",
    "    \"Cross-attn dim\": cross_attention_dim,\n",
    "    \"Total params\": total_params,\n",
    "    \"Trainable (LoRA) params\": trainable_params,\n",
    "    \"Trainable %\": trainable_pct\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364cd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
